# --- Transfer with SF specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 0.5
epsilon_finish: 0.0
epsilon_anneal_time: 50000

runner: "mt_episode" # the same way to handle multi-task info in runners

buffer_size: 5000

# update the target network every {} episodes
target_update_interval_or_tau: 0.005

# learner />
agent_output_type: "q" # regard psi as q'ds
learner: "tr_sf_learner"
## for pretrain
r_lambda: 0.001
w_range_reg: 0.0001
w_range_gate: 30 # w_reg lower bound == 2ï¼Œ lbf:10, smacv1v2:20
## for RL
double_q: True
w_std_lambda: 0.1
# mixer: tr_dmaq_qatten
mixer: tr_qatten
cql_alpha: 0.1 # largely depend on the quality of dataset
cql_type: "base"
# use phi as reward model: ratio alpha_t = 1 - exp(-rew_beta * t / rew_step)
rew_beta: 5
rew_step: 500000

# hypernet
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

# /> for LBF
# checkpoint_path: /home/amax/xyc/MATr/offpymarl/results/transfer/lbforaging/lbf_test/tr_sf/seed_5_tr_sf_2025-04-16_11-06-45/models/offline/
# load_step: 10005
# </ for LBF

# /> for smac
# checkpoint_path: /home/amax/xyc/MATr/offpymarl/results/transfer/sc2/sc2_test/3m-expert+4m-expert/tr_sf/seed_2_tr_sf_2025-03-30_10-20-46/models/offline
# load_step: 50005
# </ for smac

# /> for smacv2
# </ for smacv2

ckpt_stage: 2
online_train_steps: 2000005 # ~40000updates
test_interval: 10000
log_interval: 10000
runner_log_interval: 10000
learner_log_interval: 10000 # ~200updates (50traj_len x 200)
save_model_interval: 500000

# </ learner

# agent type
agent: "tr_sf_rnn"
phi_dim: 64
invdyn_cond_type: "cat"

# mac
mac: "tr_basic_mac"

standardise_returns: False
standardise_rewards: True

# params about observation decomposition
id_length: 4
max_agent: 15

# params about mt mixing network
entity_embed_dim: 64
attn_embed_dim: 8
head: 1

name: "tr_sf"
